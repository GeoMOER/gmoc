[
["index.html", "Advanced Programming in R Preface", " Advanced Programming in R Florian Detsch Last modified: 2017-11-05 Preface The following R tutorial is not for beginners. This might seem odd at first since most tutorials tend to start with the very basics of the language, e.g. 1 + 1 ## [1] 2 or, when it comes to using variables instead of calculator-style raw numbers, a &lt;- 1 b &lt;- 1 a + b ## [1] 2 Well, let’s assume you are already familiar with such things. In fact, maybe it is because of the overwhelming number of R tutorials on the web that this particular tutorial starts a little later, after the first excitement has settled and one has grown familiar with the syntax and the basic commands. All well and good, of course, but soon you will notice that you could use a little more of the stuff you heard R was capable of. There is, for instance, the saying that using for loops in R is not quite convenient - as opposed to other programming languages like C++. So how is that, and what should I use as an alternative? The same possibly applies for base graphics. Sure thing, everyone of us has been excited at some stage about the professional default style of R’s artwork, not least because of the rather awkward MS Excel style that is still widely applied in the field of academic research, among others. Well, decide for yourselves which of the following figures – that were both created using (almost) default settings only – is more appealing for you. It could also be conceivable that you came across a work step that required massive computational power, and thus took quite long to finish. R, by default, uses only one core on your machine, which could likely be the reason for the slow performance of your code. If you haven’t heard, you can actually tell R to use multiple cores to reasonably split up slow peaces of code and make certain operations perform much faster. Today, most machines come with at least 4 internal cores which makes it easy to do such stuff. You see, there’s plenty of applications that definitely go beyond the scope of a base-R programmer’s skills. In case you’ve felt a continuously rising interest while reading this short introduction, you might want to browse the following chapters of this tutorial on ‘Advanced Programming in R’ and see for yourselves. Some technical remarks before starting out: This tutorial is published under the creative commons license ‘Attribution-ShareAlike 3.0 Unported (CC BY-SA 3.0)’. It has constantly evolved over the years and has lately become an annual course in the event calendar of the MArburg University Research Academy (MARA). Of course, comments, feedback, suggestions and bug reports are always welcome and should be directed to florian.detsch{at}staff.uni-marburg.de. Please note that we adhere to the format conventions introduced by Xie (2016) in his introductory book about the R bookdown package. Accordingly, we do not add prompts (&gt; and +) to the R source code presented herein, and text output is commented out with two hashes (##) by default, for example: cat(&quot;Hello world.\\n&quot;) ## Hello world. This is meant to facilitate copying and running the code, while the text output will be automatically ignored since it is commented out. Package names are in bold text (e.g., grid), and inline code and file names are formatted in a typewriter font (e.g., grid::upViewport(0)). Function names are followed by parentheses (e.g., grid::viewport()). The double-colon operator :: means accessing a function from a particular package. Enough talking now, let’s dive into the coding work! References "],
["towards-a-structured-workflow.html", "1 Towards a structured workflow", " 1 Towards a structured workflow What is it actually? Briefly put, the meaning of ‘workflow’ in a general sense may be summarized as “[…] an abstraction of an automated and computerized business process. It consists of a set of activities that are interconnected by control flows […]. Each activity is a naturally defined task in a workflow and has associated servers that are either humans or executions of programs commonly called processes […].” (Son and Kim 2001) Accordingly, from an R perspective, the term typically describes a set of interconnected scripts and functions which sequentially perform individual tasks to generate a desired result. In the majority of cases, such workflows rely on certain input data based on which some statistical metrics or figures shall be produced. What’s the use of it? This section will introduce you to two fundamental pillars of a structured workflow in R, namely Git version control in Section 1.1 and RStudio projects in Section 1.2. While the former is meant to separate the single projects you are working on and, at the same time, keep your stuff together in unique project-related working directories, the latter helps you to keep track of your single R code snippets and, more precisely, the changes you (or somebody else) made over time. References "],
["git.html", "1.1 Git version control", " 1.1 Git version control What is it actually? GitHub is an online interface which hosts and provides access to the Git version control system. Briefly, the software enables multiple users to work on a single (code) project, or workflow, without the need to share a common network. Following the principles of distributed revision control, each change to a file or folder is recorded, thus making any edit you’ve made comprehensible for other developers – and also making it easy to undo changes if something went terribly wrong! For example, a short web-based tutorial to get started with the program is available from Try Git. At this point, we would like to invite you to take this roughly ten-minute tour to get used to the basic git commands that we are going to use repeatedly during the subsequent lessons. Initialize a new online code repository Let’s get back to our own personal R business now. In order to be able to add version control to an RStudio project later on, you are required to create a user account at GitHub. This involves the specification of a username, e-mail address, and password only and goes without giving away personal data. Once your account has been created and you’ve successfully logged in, select ‘New repository’ from the ‘Create new…’ (plus sign) drop-down menu in the top-right corner to create a new GitHub code repository. Now, enter ‘advanced-programming-in-r’ as repository name and, optionally, provide a short description of the data it contains (or, more precisely, the data that it is supposed to contain in the future). Finally, hit the ‘Create repository’ button to complete the setup process. So far, the code repository thus created is available online only. In order to get a local copy on your hard disk, copy the link to be found when hitting the green ‘Clone or download’ button on the main webpage of your GitHub repository. Next, open up a Git Bash or Terminal (depending on your operating system) and navigate to the folder where you intend to save local copies of your Git repositories in the future. Finally, run git clone https://github.com/fdetsch/advanced-programming-in-r.git (remember to replace the name) et voilà, all files associated with your online code repository are being downloaded to your destination folder. From now on, this will be the place for you to realize your coding work and, at the end of a day’s work, commit any changes to the online repository to better keep track of the latest changes to your code. Task: Create a local SSH key Before we can push files to the newly created online code repository, we have to create a SSH key (if not already present) for the local machine you are currently working on. For this purpose, follow the official GitHub tutorial on Generating SSH keys and, if necessary, create a SSH key locally and add it in your online account settings. Now that you successfully set up your (maybe first) own code repository, it is time to move on to the second crucial step towards establishing a consistently structured workflow in R: Setting up an RStudio Project and trace your tedious coding work using Git version control. "],
["rproj.html", "1.2 RStudio projects", " 1.2 RStudio projects RStudio projects (.Rproj) are a handy solution to keep all the files related to a specific workflow, including R scripts (.R), workspace (.RData), history (.Rhistory), datasets, etc., together in one working directory. RStudio projects, once opened, are automatically associated with the directory the .Rproj file is located in, thus rendering unnecessary the use of setwd(). Starting a new project From inside RStudio, projects are easily initialized and associated with either a brand new working directory, an existing directory or a local copy of a version control repository (of which more later on) by hitting the ‘New Project…’ button from the drop-down menu at the top-right corner. Working with projects Once created, RStudio automatically switches to the new project (not without asking if you’d like to safe your latest changes, of course). If you desire to work with an already existing project instead, you may simply open the referring .Rproj file either via the drop-down menu in the top-right corner, or just double-click it within your system’s file explorer. Note that, among others, the following actions are taken when a project opens inside RStudio (taken directly from the RStudio Support Team: a new R session is started; the .Rprofile in the project’s main directory (if any) is sourced; the recent workspace stored in .RData in the project’s main directory is loaded (if indicated by the project options); the .Rhistory file in the project’s main directory is loaded into the RStudio ‘History’ pane and available for console up/down arrow command history; previously edited source documents are restored into editor tabs. Similarly, when you quit a project, the .RData and .Rhistory are being saved (if indicated by the project options); the currently active R scripts are being saved; the current R session is terminated. Task: Create your own RStudio project Now it’s your turn: Initialize a new RStudio project in the advanced-programming-in-r subfolder from Section 1.1, i.e. the one with Git version control enabled! For that purpose, choose an existing directory rather than a new one and navigate to the advanced-programming-in-r subfolder that you have copied from GitHub earlier using git clone. After having accomplished this, git add all the stuff that has accumulated in the local project directory so far (mostly .Rproj related files) followed by git commit (including a meaningful description of your recent actions) and, finally, git push. It is totally up to you whether to use the built-in RStudio interface to be found in the top-right ‘Git’ pane or run git from the command line. Due to its ease of use, I highly encourage you to choose option #1, though. To verify that everything worked as expected, head over to the web-based representation of your code repository on GitHub. In addition, don’t forget that you may use git status at any stage to get further information on the status quo of the Git processing chain. There’s also a comprehensive tutorial on version control with Git (and SVN) on the RStudio support website if you wish to gather further insights into the topic. "],
["data-handling-and-visualization.html", "2 Data handling and visualization", " 2 Data handling and visualization As noted earlier in this guide, an R workflow basically consists of performing calculations and/or visualizing your data which has previously been imported into R. In the majority of cases, this workflow usually includes an intermediary step referred to as data manipulation meant to get your data into shape for visualization. Note that some packages even require you to provide the data to be displayed in a particular format (of which more later on). Within the scope of this short course, we’ll simply assume that you already heard a lot about importing your data into R. Even if this were not the case, there’s plenty of tutorials on the internet demonstrating the use of read.table() and the like. Help on this topic may be found e.g. on Quick-R, CRAN, R-Tutor and so forth. Just remember, Google is your friend ☺ Data manipulation An overview of typical data manipulation steps, including illustrative examples, is provided by the R Cookbook and includes, among others, General operations Sorting Converting between vector types Dealing with duplicate records and missing values Factor operations Renaming and re-computing factor levels Reordering factor levels (which comes in quite handy when plotting data) Data frame operations Adding and removing columns Reordering columns Merging data frames Restructuring data Data format conversions Summarizing data Since I assume that you are already familiar with the basics of data manipulation, we will primarily focus on the latter point which represents an essential step towards conveying your results clearly and vividly to a broader audience. The diamonds dataset In the following subsections, I’ll introduce the essentials of data restructuring in R on the basis of the diamonds dataset, which is part of the ggplot2 package. In case you are not already familiar with the dataset, just take a minute and have a look at ?diamonds in order to get a more detailed description of the single variables. Now, it is important to realize that diamonds is not a standard R data.frame (in fact, it used to be one), but an object of class tbl_df. library(ggplot2) class(diamonds) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; One good thing about such objects in R is that their print() method largely renders ritual-like head(), tail(), or str() calls unnecessary. diamonds ## # A tibble: 53,940 x 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 ## 4 0.29 Premium I VS2 62.4 58 334 4.20 4.23 2.63 ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 ## 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 ## 7 0.24 Very Good I VVS1 62.3 57 336 3.95 3.98 2.47 ## 8 0.26 Very Good H SI1 61.9 55 337 4.07 4.11 2.53 ## 9 0.22 Fair E VS2 65.1 61 337 3.87 3.78 2.49 ## 10 0.23 Very Good H VS1 59.4 61 338 4.00 4.05 2.39 ## # ... with 53,930 more rows Notice that not only the class and the dimensions of the dataset are displayed, but also the variable types of the single columns. Furthermore, the console output is truncated after the first 10 rows (which would also apply to the displayed columns if we were dealing with a somewhat wider dataset). Practically, this means that the appropriate use of tbl_df lets you kill two birds with one stone compared with the 2-step approach via head and str that is usually required for investigating standard data frames. "],
["summarizing-data.html", "2.1 Summarizing data", " 2.1 Summarizing data The dplyr package is a marvelous and very intuitive set of tools to perform fast and easy data manipulation and summarization in preparation e.g. for visualization. In the following, we’ll guide you through a selection of operations you may find helpful within the course of future R workflows on the basis of the diamonds dataset. Based on a very comprehensive RPubs tutorial, these include filter(), select(), arrange(), mutate(), and summarise() (incl. group_by()), each of which will be accompanied by an equivalent base-R approach. Subsetting data via filter() Just like base-R subset() or simply indexing via conditional expressions included in square brackets, filter() creates a subset of your data based on some user-defined criteria. Before we dive any deeper into this, let’s check your subsetting abilities. Task: base-R subsetting Suppose we want to create a subset of diamonds keeping all D-, E- or F-colored specimen with a ‘Premium’ or ‘Ideal’ cut quality and a weight of more than 3 carat. Try to figure out two different solutions, one with and one without using subset. Using filter() is straightforward and very similar to subset(). The first argument represents the dataset under investigation, whereas any subsequent argument represents a logical expression to filter particular rows. Note that in contrast to the direct subsetting via square brackets, you are not required to repeat the name of the dataset with every single condition. library(dplyr) filter(diamonds, carat &gt; 3 &amp; cut %in% c(&quot;Premium&quot;, &quot;Ideal&quot;) &amp; color %in% c(&quot;D&quot;, &quot;E&quot;, &quot;F&quot;)) ## # A tibble: 2 x 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3.01 Premium F I1 62.2 56 9925 9.24 9.13 5.73 ## 2 3.05 Premium E I1 60.9 58 10453 9.26 9.25 5.66 Selecting columns via select() Selecting specific columns from a dataset follows a similar syntax to filter() and works analogous to SQL standards. Again, the first argument represents the dataset under consideration followed by the desired column names (without double quotes). dplyr::select(diamonds, carat, cut, color, clarity) ## # A tibble: 53,940 x 4 ## carat cut color clarity ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; ## 1 0.23 Ideal E SI2 ## 2 0.21 Premium E SI1 ## 3 0.23 Good E VS1 ## 4 0.29 Premium I VS2 ## 5 0.31 Good J SI2 ## 6 0.24 Very Good J VVS2 ## 7 0.24 Very Good I VVS1 ## 8 0.26 Very Good H SI1 ## 9 0.22 Fair E VS2 ## 10 0.23 Very Good H VS1 ## # ... with 53,930 more rows Note that both the use of c() to combine the single column names into a vector as well as the need for double quotes became obsolete. In addition to this, column names may be treated analogous to numeric indices, eliminating the need to count columns when desiring to extract several consecutive columns from a rather wide datasets. dplyr::select(diamonds, carat:clarity, price) ## # A tibble: 53,940 x 5 ## carat cut color clarity price ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;int&gt; ## 1 0.23 Ideal E SI2 326 ## 2 0.21 Premium E SI1 326 ## 3 0.23 Good E VS1 327 ## 4 0.29 Premium I VS2 334 ## 5 0.31 Good J SI2 335 ## 6 0.24 Very Good J VVS2 336 ## 7 0.24 Very Good I VVS1 336 ## 8 0.26 Very Good H SI1 337 ## 9 0.22 Fair E VS2 337 ## 10 0.23 Very Good H VS1 338 ## # ... with 53,930 more rows There’s also a set of additional helper functions, including starts_with(), ends_with(), matches() and contains(), which definitely go beyond the scope of this short introduction. Still, it’s good to know that such things existed in case you needed any of them. A brief note on chaining (or pipelining) Now suppose we wanted to select the same columns from the previously created subset of data. Traditionally, this would either require a 2-step approach, generating otherwise unnecessary intermediate results, via ## first, create subset diamonds_sub &lt;- filter(diamonds, carat &gt; 3 &amp; cut %in% c(&quot;Premium&quot;, &quot;Ideal&quot;) &amp; color %in% c(&quot;D&quot;, &quot;E&quot;, &quot;F&quot;)) ## second, select columns select(diamonds_sub, carat:clarity, price) or nested function calls which are usually hard to read. ## all-in-one nested solution select( filter(diamonds, carat &gt; 3 &amp; cut %in% c(&quot;Premium&quot;, &quot;Ideal&quot;) &amp; color %in% c(&quot;D&quot;, &quot;E&quot;, &quot;F&quot;)), carat:clarity, price ) dplyr introduces the %&gt;% operator which is meant to bridge a set of connected processing steps, thus eliminating the need for intermediary variables (e.g. diamonds_sub in the above example) or nested function calls. Just think of %&gt;% as a “then” connecting two parts of a sentence. diamonds %&gt;% filter(carat &gt; 3 &amp; cut %in% c(&quot;Premium&quot;, &quot;Ideal&quot;) &amp; color %in% c(&quot;D&quot;, &quot;E&quot;, &quot;F&quot;)) %&gt;% dplyr::select(carat:clarity, price) ## # A tibble: 2 x 5 ## carat cut color clarity price ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;int&gt; ## 1 3.01 Premium F I1 9925 ## 2 3.05 Premium E I1 10453 Note that chaining comes in handy when performing multiple operations at once, rendering your code much more elegant and reducing the accumulated overhead significantly. The single worksteps can be read from left to right and from top to bottom, just like you would read the pages of a book. Reordering rows via arrange() arrange() ensures a fast and easy rearrangement of rows based on certain variables. Although this can be done using base-R, its necessity soon becomes clear when dealing with rearrangements based on multiple variables as the dplyr approach requires far less typing. Task: base-R rearrangement of rows Here’s a tricky one. Let’s assume we wanted to rearrange the rows of a subset of diamonds (color, price, carat) according to color, with the best color (D) on top. Since we’re also interested in a preferably low price, the diamonds of uniform color should be sorted according to their price, with cheapest ones on top. Finally, for each set of diamonds with a specified color and price, we want their weights arranged in descending order, with the heaviest specimen on top. Again, the dplyr approach is much simpler and can be done in one go. diamonds %&gt;% dplyr::select(color, price, carat) %&gt;% arrange(color, price, desc(carat)) ## # A tibble: 53,940 x 3 ## color price carat ## &lt;ord&gt; &lt;int&gt; &lt;dbl&gt; ## 1 D 357 0.23 ## 2 D 357 0.23 ## 3 D 361 0.32 ## 4 D 362 0.23 ## 5 D 367 0.24 ## 6 D 367 0.20 ## 7 D 367 0.20 ## 8 D 367 0.20 ## 9 D 373 0.24 ## 10 D 373 0.23 ## # ... with 53,930 more rows Adding new columns via mutate() mutate() lets you add new variables to an existing data frame. It is basically an extended version of base-R transform() in the sense that it allows you to directly work with columns you’ve just created. diamonds %&gt;% dplyr::select(color, carat, price) %&gt;% mutate(ppc = price / carat, ppc_rnd = round(ppc, 2)) ## # A tibble: 53,940 x 5 ## color carat price ppc ppc_rnd ## &lt;ord&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 E 0.23 326 1417.391 1417.39 ## 2 E 0.21 326 1552.381 1552.38 ## 3 E 0.23 327 1421.739 1421.74 ## 4 I 0.29 334 1151.724 1151.72 ## 5 J 0.31 335 1080.645 1080.65 ## 6 J 0.24 336 1400.000 1400.00 ## 7 I 0.24 336 1400.000 1400.00 ## 8 H 0.26 337 1296.154 1296.15 ## 9 E 0.22 337 1531.818 1531.82 ## 10 H 0.23 338 1469.565 1469.57 ## # ... with 53,930 more rows Summarise values via summarise() Similar to plyr::summarise(), the dplyr version of summarise() lets you melt a dataset into a single row depending on the supplied function. The function works quite similar to base-R aggregate(). However, the ease of use is definitely on the side of summarise(). Task: base-R data aggregation In order to demonstrate dplyr’s ease of use in terms of data summarization, try to aggregate() the diamonds dataset in such a fashion that you end up with a data frame showing the minimum, mean and maximum price per diamond color. As for the dplyr solution, the group_by function comes in handy when trying to calculate metrics from sub-groups of data, e.g. depending on the diamonds’ color, rather than from the entire data frame. diamonds %&gt;% group_by(color) %&gt;% summarise(MIN = min(price), MEAN = mean(price), MAX = max(price)) ## # A tibble: 7 x 4 ## color MIN MEAN MAX ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 D 357 3169.954 18693 ## 2 E 326 3076.752 18731 ## 3 F 342 3724.886 18791 ## 4 G 354 3999.136 18818 ## 5 H 337 4486.669 18803 ## 6 I 334 5091.875 18823 ## 7 J 335 5323.818 18710 In order to summarize an entire variable into one single value, just drop the group_by function. summarise(diamonds, min = min(price), mean = mean(price), max = max(price)) ## # A tibble: 1 x 3 ## min mean max ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 326 3932.8 18823 Now that you’re familiar with the basics of data manipulation via dplyr and before moving on to actual data visualization via ggplot2, it’s time to have a brief look at data format conversion from wide to long format which is essential for displaying numerous groups of data in one single plot. "],
["data-format-conversions.html", "2.2 Data format conversions", " 2.2 Data format conversions Usually, your data is arranged in matrix format with rows and columns representing observations and variables, respectively. This is the common case not only for R, but also for many other statistical software packages, including Excel and SPSS. However, certain R packages, including visualization via ggplot2, are easier to handle when using a long data format rather than a wide one. Therefore, a “rearrangement of the form, but not the content, of the data” (Wickham 2007) according to so-called identifier (or ID) and measured variables becomes necessary. Here’s a simple example taken from the Cookbook for R to illustrate this point. Let’s start off with the wide format: olddata_wide &lt;- read.table(header = TRUE, text = &#39; subject sex control cond1 cond2 1 M 7.9 12.3 10.7 2 F 6.3 10.6 11.1 3 F 9.5 13.1 13.8 4 M 11.5 13.4 12.9 &#39;) Notice the strict structure of the data.frame with observations of subjects 1-4 (two males and two females) arranged in rows and measured variables (‘control’, ‘cond1’, ‘cond2’) in columns? Now, let’s see what the long version of the dataset would look like: library(reshape2) melt(olddata_wide, id.vars = c(&quot;subject&quot;, &quot;sex&quot;)) ## subject sex variable value ## 1 1 M control 7.9 ## 2 2 F control 6.3 ## 3 3 F control 9.5 ## 4 4 M control 11.5 ## 5 1 M cond1 12.3 ## 6 2 F cond1 10.6 ## 7 3 F cond1 13.1 ## 8 4 M cond1 13.4 ## 9 1 M cond2 10.7 ## 10 2 F cond2 11.1 ## 11 3 F cond2 13.8 ## 12 4 M cond2 12.9 What just happened? melt() (or, more precisely, melt.data.frame()) from the reshape2 package was cast upon the dataset, forcing an increase in the number of rows to the expense of columns. Columns ‘subject’ and ‘sex’ were thereby specified as ID variables, which prevented them from being split apart. The remaining variables, on the other hand, now got arranged among each other rather than next to each other, with each row representing a unique ID-variable combination. You will notice later on that such a format is much easier to handle when it comes to visualizing grouped variables. Task: Melting diamonds The diamonds dataset included in ggplot2 features quite a variety of variables per specimen. For example, let’s assume we want to convert the dataset from wide into long format, thus resulting in a significant reduction of columns. Since no ‘true’ ID column is included so far, create a new column ‘ID’ from rownames(diamonds) and subsequently melt() the dataset using all factor columns and the newly established ID column as id.vars. References "],
["visualizing-data-using-ggplot2.html", "2.3 Visualizing data using ggplot2", " 2.3 Visualizing data using ggplot2 Now that you’ve grown familiar with the basics of data manipulation using dplyr and reshape2, the next logical step in our workflow is to visualize the data that we’ve exhaustively prepared. Be aware that there are plenty of marvellous graphics packages available in R depending on which tasks you’d like to perform. Within the scope of our short course, ggplot2 will do just fine not least because it is closely linked to the previously performed manipulation tasks (maybe because of the same author). In ggplot2, every plot we want to draw consists of incremental calls on how to represent the individual plot components (basically x and y). This means a much more consistent way of building visualizations as compared, for instance, to the much more conventional style of lattice, but it also means that things are rather different from what you might have learned about syntax and structure of (plotting) objects in R so far. Note also that the brief examples shown below are mainly taken from another MARA course about Creating publication quality graphs using R. Feel free to browse the code tutorial or the accompanying course slides in case you’re not entirely satisfied with the rather shallow introduction given here or if you wish to learn more about lattice-style figures. Scatter plots Scatter plots are produced by invoking geom_points(). If we wanted to plot variables ‘price’ and ‘carat’ from diamonds against each other, the ggplot() call would be as follows. library(ggplot2) ggplot(aes(x = carat, y = price), data = diamonds) + geom_point() Figure 2.1: A basic scatter plot created with ggplot2. Let’s look at the above code in a little more detail. The first line is the fundamental definition of what we want to plot. We provide the ‘aesthetics’ for the plot (aes()) and state that we want the values on the x-axis (y-axis) to represent ‘carat’ (‘price’). The data to take these variables from is our diamonds dataset. That’s basically it, and this will not change a hell of a lot in the subsequent plotting routines. What will change in the code chunks that follow is how we want the relationship between these variables to be represented in our plot. This is done by defining so-called geometries (geom_*()). In this first case we stated that we want the relationship between x and y to be represented as points, hence we used geom_point(). For the sake of practicing, let’s add another layer to our plot. In order to provide the regression line, for instance, we need a function called stat_smooth(). Note that we also specify the smoothing method and the line colour, size and linetype. se = FALSE tells the function not to display confidence intervals for the moment. See ?stat_smooth for a more detailed overview of costumization possibilities. ggplot(aes(x = carat, y = price), data = diamonds) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE, colour = &quot;red&quot;, size = 1.5, linetype = &quot;dashed&quot;) Figure 2.2: A ggplot2 scatter plot incl. regression line. If we wanted to provide a plot showing the relationship between ‘price’ and ‘carat’ in panels representing the quality of the diamonds, we need what in ggplot2 is called facetting (similar to panels in lattice). To achive this, we simply repeat our plotting call from earlier and add another layer to the call which does the facetting. Note that this time, confidence intervals are included via se = TRUE and fill-ed grey. ggplot(aes(x = carat, y = price), data = diamonds) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = TRUE, colour = &quot;red&quot;, fill = &quot;grey35&quot;) + facet_wrap(~ cut) Figure 2.3: A facetted scatter plot created with ggplot2. One thing that some people dislike about the default settings in ggplot2 is the grey background of the plots. This grey background is, in my opinion, a good idea when colors are involved as it tends to increase the contrast of the colors. If, however, the plot is a simple black-and-white scatter plot, a white facet background seems more reasonable. We can easily change this using a pre-defined theme called theme_bw(). Note that in the following, we also change the axis titles (labs()) and the number of rows (nrow) and columns (ncol) into which the facets should be arranged – simple and straightforward, isnt’t it? ggplot(aes(x = carat, y = price), data = diamonds) + geom_point(colour = &quot;grey65&quot;) + stat_smooth(method = &quot;lm&quot;, se = TRUE, colour = &quot;black&quot;, fill = &quot;grey35&quot;) + facet_wrap(~ cut, nrow = 2, ncol = 3) + labs(x = &quot;Carat&quot;, y = &quot;Price ($)&quot;) + theme_bw() Figure 2.4: A facetted scatter plot created with ggplot2 and using a modified theme. Box and whisker plots Box and whisker plots (in the following ‘boxplots’) drawn with ggplot2 look quite nice and, in contrast to lattice, do not require the user to exhaustively modify graphical parameter settings in order to get an acceptable result. But see for yourselves (using the default settings). ## basic frame p &lt;- ggplot(aes(x = color, y = price), data = diamonds) ## add boxplot p_bw &lt;- p + geom_boxplot() ## print print(p_bw) Figure 2.5: A basic boxplot created with ggplot2. Note that this time, we stored the basic frame of our plot in a variable p and added the desired geom_boxplot() layer afterwards. In order to keep a clear structure, this is possibly the ideal way to create graphics using ggplot2 - so stick to it! As we’ve already seen, facetting and modifying the default theme require also just one more line of code each. ## add faceted boxplots p_bw &lt;- p + geom_boxplot(fill = &quot;grey90&quot;) + facet_wrap(~ cut) + theme_bw() print(p_bw) Figure 2.6: A facetted boxplot created with ggplot2 and using a modified theme. Histograms and density plots Much like with boxplots, ggplot2 produces quite nice histograms and density plots when using the default settings. ## new basic frame p &lt;- ggplot(aes(x = price), data = diamonds) ## add histogram p_hist &lt;- p + geom_histogram() print(p_hist) Figure 2.7: A basic histogram produced with ggplot2. When working with density plots in ggplot2, it is quite easy to fill the area under the curve which really contributes to the visual representation of the data. ## add densityplot p_dens &lt;- p + geom_density(fill = &quot;black&quot;, alpha = 0.5) print(p_dens) Figure 2.8: A basic density plot produced with ggplot2 In addition to theme_bw(), there’s a lot of different theme() settings that let you customize your plots at will. For instance, let us slightly rotate the tick labels on the x-axis and move them horizontally a little bit. element_text() is an essential function to achieve such things as it allows you to modify, among others, font family and face (e.g. ‘bold’), text colour and size, horizontal (hjust) and vertical (vjust) justification as well as angle of the tick labels. (In case you prefer American English, element_text() – or for that matter any ggplot2 function – also understands color instead of colour ☺) p_dens &lt;- p + geom_density(fill = &quot;black&quot;, alpha = 0.5) + facet_grid(color ~ cut) + theme_bw() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) print(p_dens) Figure 2.9: A facetted density plot conditioned according to 2 variables created with ggplot2. "],
["iteration-procedures.html", "3 Iteration procedures", " 3 Iteration procedures In computer programming the term iterative process usually refers to some block of code being repeated more than once to achieve a result. Two terms are frequently used in this context: Iteration Recursion In this part of the tutorial we will have a closer look at Iteration and how to achieve it in numerous ways in R. In general, Iteration executes a certain function or combination of functions repeatedly until some condition is met. The classical for loop is common in pretty much all programming languages, while *apply is pretty exclusive to R (except for Pandas in Python). Finally, given that R is a so-called functional programming language, we will also see how we can use ‘functionals’ and ‘closures’ which are very elegant and flexible structures to iteratively achieve a certain aim. Recursion is another aspect of repeated computation, however, we will not delve into any detail on this topic here. In contrast to Iteration, recursive structures define functions that call themselves until a certain condition is met. Below is a classical example of a recursive function, the so-called ‘quicksort’ algorithm. This function sorts a numerical vector from small to large values by first selecting an arbitrary value from the vector (in the below case the first) called the pivot, rearranging the remainding values so that all values smaller than pivot come before it and all larger values after it, and repeating this procedure within the function by calling itself for either side of the pivot. Once there are only single values left on either side of the pivot, the function stops. Note, without the inherent stopping condition this function will run for eternity. This particular example of quicksort is adapted from Rosetta Code qsort &lt;- function(v) { if (length(v) &gt; 1) { pivot &lt;- v[1] c(qsort(v[v &lt; pivot]), v[v == pivot], qsort(v[v &gt; pivot])) } else v } qsort(rnorm(10)) ## [1] -1.50303478 -1.31274507 -1.27806229 -0.02375993 0.05077221 ## [6] 0.05320810 0.34306148 1.25132005 1.39315258 1.67604230 Because they call themselves, recursive functions are inherently hard to code, understand and debug. Therefore, we leave it at this small example and continue with the more understandable structures of iterative comutation, namely for() loops, *apply functions and ‘functional programming’. "],
["for.html", "3.1 for() loops", " 3.1 for() loops Maybe the most important part of any advanced analysis workflow is to avoid (code) repetition. If you need to perform a certain task many times, proper interation structures are far more desirable than the classical copy-and-paste approach. Why? Because they help you avoid errors (as each copy of any code chunk increases the risk of error introduction); make debugging a lot easier as you only have to debug once; make your code a lot shorter and more readable; save you time coding; etc. The classical iteration structure is the so-called for loop. Classic, because it is the back-bone of any and all programming languages. The basic concept is simple for (a certain amount of iterations) do this The way the iteration sequence is specified differs between languages, but the basic principle is always the same. The sequence is usually specified via some placeholder – most popular of which is i for iteration – which usually represents an integer sequence, though you can also iterate over character strings. Here’s a very simple loop: for (i in 1:5) print(i) ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 Make sure you code clearly so that you can easily understand what your code is doing. Therefore, when using iteration structures, try to provide meaningful placeholder names. for (name in names(diamonds)) print(name) ## [1] &quot;carat&quot; ## [1] &quot;cut&quot; ## [1] &quot;color&quot; ## [1] &quot;clarity&quot; ## [1] &quot;depth&quot; ## [1] &quot;table&quot; ## [1] &quot;price&quot; ## [1] &quot;x&quot; ## [1] &quot;y&quot; ## [1] &quot;z&quot; Another way to specify an iteration is via the seq() function which can be used to create a sequence of integers based on the length of an object. for (i in seq(names(diamonds))) cat(&quot;The class of colom&quot;, i, &quot;is&quot;, class(data.frame(diamonds)[, i]), &quot;\\n&quot;) ## The class of colom 1 is numeric ## The class of colom 2 is ordered factor ## The class of colom 3 is ordered factor ## The class of colom 4 is ordered factor ## The class of colom 5 is numeric ## The class of colom 6 is numeric ## The class of colom 7 is integer ## The class of colom 8 is numeric ## The class of colom 9 is numeric ## The class of colom 10 is numeric Did you spot the spelling mistake in the above example? It is easy to debug this, as we only need to correct the error once, not 10 times. Task: Looping the mean, meaning the loop Take the above for loop and modify so that instead of the class it prints out the mean of each column. Obviously, we are usually not interested in simply printing something to the console (though this can be a great way of keeping track of where you’re computation is in case of long running loops). Most of the time we want to actually run some computations/statistical analyses. The principle remains the same. You may have heard the notion that for() loops in R are slow and should be avoided. This is true in many situations. As a general rule, you should avoid for() loops whenever you want to do calculations on parts of an object. This can be achieved much more efficiently with indexing in R if you have multiple objects and you want to carry out the same operation on each of them or you have one object and want to carry out different types of calculations on this same object, there is nothing wrong with using for() loops. One common scenario is to work with different data sources. Here, for() loops can be quite useful. To highlight this, let’s create a few data sources and save them to disk (we will load these again later). Instead of reusing the same lines of code, we dynamically subset the data, build suitable names for saving and finally save each part of the data. ## index for start and end rows to be extracted indx_start &lt;- seq(1, nrow(diamonds), 2000) indx_end &lt;- c(indx_start[-1] - 1, nrow(diamonds)) ## create new directory to save files to dir_nm &lt;- &quot;results&quot; dir.create(dir_nm, showWarnings = FALSE) ## looping through the files for (i in seq(indx_start)) { ## actual indeces for current iteration st &lt;- indx_start[i] nd &lt;- indx_end[i] ## subset data based on row indeces dat &lt;- diamonds[st:nd, ] ## create unique name for iteration data nm &lt;- paste0(dir_nm, &quot;/&quot;, &quot;diamonds_subset_&quot;, sprintf(&quot;%05.0f&quot;, st), &quot;_&quot;, sprintf(&quot;%05.0f&quot;, nd), &quot;.csv&quot;) ## write to disk write.csv(dat, nm, row.names = FALSE) } for() loops are great for iterative operations that do not require assignment of their output to an object, e.g. the example above of saving data or producing plots. If we want the outcome of a loop to be assigned to an object and use this for further analysis, R has much more convenient structures which we will see in the next chapter. Finally, there are more pieces related to iteration procedures in R: while() loops to do something while some condition is met (e.g. while a certain value is below a certain threshold or the like). break allows you to create a condition so that once it is met, the loop will stop. next allows you to create a condition so that if it is met, the execution of the current iteration is skipped and the loop procedes to the next iteration without breaking the loop. Finally, if-statements can also be helpful to prevent errors within loops. "],
["the-apply-family.html", "3.2 The *apply family", " 3.2 The *apply family You have possibly heard of this family of functions and may have wondered what they are all about and how they differ from classic loop structures. In this course we will familiarize ourselves with apply() as well as lapply() and sapply(). First of all, we need to recognize that these structures are pretty much unique to R. The reason for this is that R is at its very core a so-called functional programming language. This means nothing else than “every operation in R is carried out by an appropriate function”. In fact, even the classical &lt;- assignment operator is defined in the same way as any other function in R. a &lt;- 3 a ## [1] 3 &quot;&lt;-&quot;(b, 15) b ## [1] 15 &quot;+&quot;(a, b) ## [1] 18 You may wonder why this is relevant to understand *apply? Well, the one thing that all *apply functions have in common is that they are all so-called functionals that can take a function as an argument. Usually, these functionals take some object and some function as input and then apply the function to every entry of the object. The difference between the various *apply functions is basically the type of object they are designed for and hence each of these has slightly different requirements for the structure of the supplied function. 3.2.1 apply() First, let’s have a look at apply(). It has three main arguments: Argument Description X A matrix (or array) or ‘clean’ data.frame that can be coerced to a matrix, i.e. no mixture of classes in the columns (see example below). MARGIN An integer specifying whether to apply the supplied function across the rows (1) or columns (2). FUN The function to be applied. Other function-related arguments such as na.rm can also be supplied. Let’s try this: ## subset diamonds to only numerical columns diamonds_num &lt;- diamonds[, -c(2:4)] ## apply function mean to all columns of diamonds col_means &lt;- apply(diamonds_num, 2, mean, na.rm = TRUE) col_means ## carat depth table price x ## 0.7979397 61.7494049 57.4571839 3932.7997219 5.7311572 ## y z ## 5.7345260 3.5387338 Task: apply sd to rows Similar to the example above, calculate the standard deviation for each row of diamonds. In R, apply() is the classical function to be used with data in matrix-like form to quickly iterate over one dimension (rows or columns). It is optimized for this kind of action and is much quicker than looping over rows or columns with for() loops which is the standard way in other languages such as Python or C++. 3.2.2 lapply() and sapply() More powerful than apply() is lapply(). The ‘l’ stands for list and simply means that whatever is returned from an iterative process will be stored in a list. In R, lists are the most flexible way of storing things but their structure may need a little getting used to. Basically, you can store any combination of objects in lists. Matrices, for example, are much less versatile. Let’s have a look at what these list objects look like: lst &lt;- list(1:10, &quot;Hello&quot;, mean, mean(1:10), function(x) x + 1, data.frame(col1 = rnorm(10), col2 = runif(10)), matrix(1:9, 3, 3)) lst ## [[1]] ## [1] 1 2 3 4 5 6 7 8 9 10 ## ## [[2]] ## [1] &quot;Hello&quot; ## ## [[3]] ## function (x, ...) ## standardGeneric(&quot;mean&quot;) ## &lt;environment: 0x0000000004e72c50&gt; ## attr(,&quot;generic&quot;) ## [1] &quot;mean&quot; ## attr(,&quot;generic&quot;)attr(,&quot;package&quot;) ## [1] &quot;base&quot; ## attr(,&quot;package&quot;) ## [1] &quot;base&quot; ## attr(,&quot;group&quot;) ## list() ## attr(,&quot;valueClass&quot;) ## character(0) ## attr(,&quot;signature&quot;) ## [1] &quot;x&quot; ## attr(,&quot;default&quot;) ## Method Definition (Class &quot;derivedDefaultMethod&quot;): ## ## function (x, ...) ## UseMethod(&quot;mean&quot;) ## &lt;bytecode: 0x0000000004e9d008&gt; ## &lt;environment: namespace:base&gt; ## ## Signatures: ## x ## target &quot;ANY&quot; ## defined &quot;ANY&quot; ## attr(,&quot;skeleton&quot;) ## (function (x, ...) ## UseMethod(&quot;mean&quot;))(x, ...) ## attr(,&quot;class&quot;) ## [1] &quot;standardGeneric&quot; ## attr(,&quot;class&quot;)attr(,&quot;package&quot;) ## [1] &quot;methods&quot; ## ## [[4]] ## [1] 5.5 ## ## [[5]] ## function (x) ## x + 1 ## &lt;environment: 0x000000002e36b488&gt; ## ## [[6]] ## col1 col2 ## 1 -0.06285000 0.6071539 ## 2 -0.94548327 0.2171210 ## 3 -0.45460562 0.3913435 ## 4 -0.81519648 0.7245288 ## 5 1.56819163 0.8119677 ## 6 0.37902423 0.5936339 ## 7 1.67091219 0.6219313 ## 8 -0.06446247 0.1376078 ## 9 -0.59308771 0.2528668 ## 10 0.48095465 0.5529387 ## ## [[7]] ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 As you can see we can combine any odd type of objects. Note how each list entry is numbered. We could have supplied names as well, but it is less common to do so if your list is the result of an interative procedure. And accessing lists via their numbered entries is quite straightforward, yet a little different from the classical $ notation of data frames. To ‘navigate’ to one of the entries we need to use double square brackets [[x]]. This is important as this will be the notion we need to keep in mind when iterating over lists. lapply() can basically be used just like a for() loop, though the semantics are a little different. The two main differences are: We can store the whole result of the lapply() call in an object (a list), and we need to write the bit that does the calculation part as a function. So, if we were to recreate the first example from the previous chapter on for() loops: result &lt;- lapply(1:5, function(i) i) result ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 2 ## ## [[3]] ## [1] 3 ## ## [[4]] ## [1] 4 ## ## [[5]] ## [1] 5 The only time we will use sapply() in this tutorial is right here. sapply() and lapply() are very similar, so that it is sufficient to cover only one in detail. The ‘s’ stands for ‘simplify’ which means that sapply() will try to return an object of simple structure, such as a vector or a matrix. Let’s repeat the above with sapply(): result &lt;- sapply(1:5, function(i) i) result ## [1] 1 2 3 4 5 Getting a vector as a result is great if the calculation produces a vector, however, this won’t work if the result is e.g. a function, a ggplot2 object or something along those lines. Therefore, lapply() is simply the more versatile of the two as it can handle any type of result. To highlight this, let’s use lapply() to read in the numerous chunks of data we have previously saved. fls &lt;- list.files(&quot;results&quot;, pattern = glob2rx(&quot;*subset*.csv&quot;), full.names = TRUE) dat_lst &lt;- lapply(seq(fls), function(i) { read.csv(fls[i]) }) str(dat_lst, 1) ## List of 27 ## $ :&#39;data.frame&#39;: 2000 obs. of 10 variables: ## $ :&#39;data.frame&#39;: 2000 obs. of 10 variables: ## $ :&#39;data.frame&#39;: 2000 obs. of 10 variables: ## $ :&#39;data.frame&#39;: 2000 obs. of 10 variables: ## $ :&#39;data.frame&#39;: 2000 obs. of 10 variables: ## $ :&#39;data.frame&#39;: 2000 obs. of 10 variables: ## $ :&#39;data.frame&#39;: 2000 obs. of 10 variables: ## $ :&#39;data.frame&#39;: 2000 obs. of 10 variables: ## $ :&#39;data.frame&#39;: 2000 obs. of 10 variables: ## $ :&#39;data.frame&#39;: 2000 obs. of 10 variables: ## $ :&#39;data.frame&#39;: 2000 obs. of 10 variables: ## $ :&#39;data.frame&#39;: 2000 obs. of 10 variables: ## $ :&#39;data.frame&#39;: 2000 obs. of 10 variables: ## $ :&#39;data.frame&#39;: 2000 obs. of 10 variables: ## $ :&#39;data.frame&#39;: 2000 obs. of 10 variables: ## $ :&#39;data.frame&#39;: 2000 obs. of 10 variables: ## $ :&#39;data.frame&#39;: 2000 obs. of 10 variables: ## $ :&#39;data.frame&#39;: 2000 obs. of 10 variables: ## $ :&#39;data.frame&#39;: 2000 obs. of 10 variables: ## $ :&#39;data.frame&#39;: 2000 obs. of 10 variables: ## $ :&#39;data.frame&#39;: 2000 obs. of 10 variables: ## $ :&#39;data.frame&#39;: 2000 obs. of 10 variables: ## $ :&#39;data.frame&#39;: 2000 obs. of 10 variables: ## $ :&#39;data.frame&#39;: 2000 obs. of 10 variables: ## $ :&#39;data.frame&#39;: 2000 obs. of 10 variables: ## $ :&#39;data.frame&#39;: 2000 obs. of 10 variables: ## $ :&#39;data.frame&#39;: 1940 obs. of 10 variables: Great, you might say, but now we have a list of multiple data frames instead of one complete data frame. In our case this seems rather silly, but think about situations where you want to analyze the same sort of data only from different dates or different locations or patients or … – you probably get the idea. Also, recombining these individual data frames back into one is straightforward. diamonds_df &lt;- do.call(&quot;rbind&quot;, dat_lst) str(diamonds_df) ## &#39;data.frame&#39;: 53940 obs. of 10 variables: ## $ carat : num 0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 ... ## $ cut : Factor w/ 5 levels &quot;Fair&quot;,&quot;Good&quot;,..: 3 4 2 4 2 5 5 5 1 5 ... ## $ color : Factor w/ 7 levels &quot;D&quot;,&quot;E&quot;,&quot;F&quot;,&quot;G&quot;,..: 2 2 2 6 7 7 6 5 2 5 ... ## $ clarity: Factor w/ 8 levels &quot;I1&quot;,&quot;IF&quot;,&quot;SI1&quot;,..: 4 3 5 6 4 8 7 3 6 5 ... ## $ depth : num 61.5 59.8 56.9 62.4 63.3 62.8 62.3 61.9 65.1 59.4 ... ## $ table : num 55 61 65 58 58 57 57 55 61 61 ... ## $ price : int 326 326 327 334 335 336 336 337 337 338 ... ## $ x : num 3.95 3.89 4.05 4.2 4.34 3.94 3.95 4.07 3.87 4 ... ## $ y : num 3.98 3.84 4.07 4.23 4.35 3.96 3.98 4.11 3.78 4.05 ... ## $ z : num 2.43 2.31 2.31 2.63 2.75 2.48 2.47 2.53 2.49 2.39 ... Fianlly, let’s look at a slightly more involved example of how to use lapply(). A standard analysis workflow likely involves some sort of statistical analysis and the visualization of the results. Here, we will create linear models between ‘carat’ and ‘price’ and the corresponding scatter plots for all levels of ‘cut’, but only for those diamonds of ‘color = D’. ## split diamonds by cut cut_lst &lt;- split(diamonds, f = diamonds$cut) my_result_list &lt;- lapply(seq(cut_lst), function(i) { ## subset to color = D dat &lt;- cut_lst[[i]] dat_d &lt;- subset(dat, dat$color == &quot;D&quot;) ## calculate linear model lm1 &lt;- lm(price ~ carat, data = dat_d) ## create scatterplot scatter_ggplot &lt;- ggplot(aes(x = carat, y = price), data = dat_d) g_sc &lt;- scatter_ggplot + geom_point(colour = &quot;grey60&quot;) + theme_bw() + stat_smooth(method = &quot;lm&quot;, se = TRUE, fill = &quot;black&quot;, colour = &quot;black&quot;) + geom_text(data = NULL, x = min(dat_d$carat, na.rm = TRUE) + 0.2, y = max(dat_d$price, na.rm = TRUE) * 0.98, label = unique(dat_d$cut)) ## return both the linear model and the plot as a list return(list(linmod = lm1, plt = g_sc)) }) ## set names of list for clarity names(my_result_list) &lt;- names(cut_lst) str(my_result_list, 2) ## List of 5 ## $ Fair :List of 2 ## ..$ linmod:List of 12 ## .. ..- attr(*, &quot;class&quot;)= chr &quot;lm&quot; ## ..$ plt :List of 9 ## .. ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;gg&quot; &quot;ggplot&quot; ## $ Good :List of 2 ## ..$ linmod:List of 12 ## .. ..- attr(*, &quot;class&quot;)= chr &quot;lm&quot; ## ..$ plt :List of 9 ## .. ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;gg&quot; &quot;ggplot&quot; ## $ Very Good:List of 2 ## ..$ linmod:List of 12 ## .. ..- attr(*, &quot;class&quot;)= chr &quot;lm&quot; ## ..$ plt :List of 9 ## .. ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;gg&quot; &quot;ggplot&quot; ## $ Premium :List of 2 ## ..$ linmod:List of 12 ## .. ..- attr(*, &quot;class&quot;)= chr &quot;lm&quot; ## ..$ plt :List of 9 ## .. ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;gg&quot; &quot;ggplot&quot; ## $ Ideal :List of 2 ## ..$ linmod:List of 12 ## .. ..- attr(*, &quot;class&quot;)= chr &quot;lm&quot; ## ..$ plt :List of 9 ## .. ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;gg&quot; &quot;ggplot&quot; This lets us now quickly access each of the analyses individually. To view the scatter plot for diamonds of ‘cut = Premium’, we simply navigate down to the respective entry: my_result_list$Premium$plt Note that we can now use the common $ notation for the navigation given that we have set the names for the resulting lists. We can, however, still navigate using double square brackets ([[]]). To get the summary of the linear model for ‘cut = Ideal’: summary(my_result_list[[5]][[1]]) ## ## Call: ## lm(formula = price ~ carat, data = dat_d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9239.6 -611.1 -38.9 419.5 10759.7 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2490.89 51.89 -48.01 &lt;2e-16 *** ## carat 9049.65 81.06 111.64 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1292 on 2832 degrees of freedom ## Multiple R-squared: 0.8148, Adjusted R-squared: 0.8148 ## F-statistic: 1.246e+04 on 1 and 2832 DF, p-value: &lt; 2.2e-16 I hope this highlights how useful and flexible lapply() can be. Another scenario that is quite common is to carry out different calculations on the same set of data. This can easily be done using lapply() by iterating over the different functions and calling them on the same data set within the lapply() loop. "],
["functional-programming.html", "3.3 Functional programming", " 3.3 Functional programming As mentioned in the previous chapter, R is at its core a functional programming language. Put more general, we can say that everything that exists is an object and everything that happens is a function call. 3.3.1 Custom functions The biggest draw-back of point-and-click statistics software is that they are usually limited in the functionality they provide. In fact, they provide you with a suite of pre-defined analysis tools and algorithms but usually it is rather tedious to extend these. R, on the other hand, is a full-blown programming language which means that there are next to no limits to what you can do. One of the most important features to expand existing functionality is to write your own functions, i.e. functions that do not exist elsewhere. In fact, this is the main reason behind R package development. So, let’s try and create a custom function. A function to calculate the Pythagorean Theorem does not exist in base R. Sure, it may exist in some package somewhere, but I argue that it is much easier and quicker to write this yourself. pythagoreanTheorem &lt;- function(a, b) { c &lt;- sqrt(a*a + b*b) return(c) } pythagoreanTheorem(3, 4) ## [1] 5 Easy! Creating custom functions thereby always follows the same procedure: Provide a meaningful name and use &lt;- (or =) to assign function(x, y, z) where x, y, z are an arbitrary number of arbitrarily named arguments that are needed for the calculation(s) in the function body that does all the calculation(s) using the supplied arguments. Finally, a return() call to specify what the function will return. If this is not supplied, the result of the last calculation is returned Now it is time for you to try this yourselves. Task: write your own function In R, we can easily calculate a population’s standard deviation around the mean using sd(), but there is no default implementation for the standard error of the mean. Therefore, it is up to you to write one now. Note, there are far more standard error statistics for which R does not provide standard base functions, such as the root mean square error (RMSE) or the absolute error (AE). Therefore, if you’re keen go ahead and practice writing functions to provide these. 3.3.2 Functionals We’ve already seen functionals, functions that take other functions as arguments. However, so far we have only used these with standard, i.e. base R functions. But we can also supply a custom function to a functional. dat &lt;- data.frame(a = c(3, 7, 11, 1, 24, 2), b = c(4, 3, 2, 3, 12, 5)) sapply(seq(nrow(dat)), function(i) pythagoreanTheorem(dat[i, 1], dat[i, 2])) ## [1] 5.000000 7.615773 11.180340 3.162278 26.832816 5.385165 Given that R comes equipped with a great variety of *apply() functionals, it is usually not necessary to write a functional yourself. 3.3.3 Closures The counterpart to functionals are so-called closures. These are functions that return (or build) a function according to some supplied argument. To illustrate this, let’s consider the following situation: We have a bunch of possible predictor variables, a bunch of response variables, and we want to figure out the best combination in explaining the variances. ### generate some random data set.seed(123) pred &lt;- data.frame(pred1 = rnorm(100, 2, 1), pred2 = 1:100, pred3 = rpois(100, 2), pred4 = 200:101) set.seed(234) resp &lt;- data.frame(resp1 = 1:100, resp2 = rnorm(100, 2, 1), resp3 = 200:101, resp4 = rpois(100, 2)) We could simply use copy and paste to claculate each combination. summary(lm(resp$resp1 ~ pred$pred1))$r.squared summary(lm(resp$resp2 ~ pred$pred1))$r.squared summary(lm(resp$resp2 ~ pred$pred1))$r.squared summary(lm(resp$resp4 ~ pred$pred1))$r.squared summary(lm(resp$resp1 ~ pred$pred2))$r.squared summary(lm(resp$resp2 ~ pred$pred2))$r.squared summary(lm(resp$resp3 ~ pred$pred2))$r.squared summary(lm(resp$resp4 ~ pred$pred3))$r.squared # ... and so forth This is far from being optimal. Despite the fact that we have to type a lot, we are very prone to introduce errors (can you spot them?) and it is particularly hard to debug. Here, defining a closure can be of great help. ### define closure calcRsq &lt;- function(pred) { function(y) { summary(lm(y ~ pred))$r.squared } } We now have a universal way of defining functions to calculate R-squared values: ## create function using pred$v1 as predictor calcRsq_pred1 &lt;- calcRsq(pred$pred1) calcRsq_pred1(resp$resp1) ## [1] 0.006369369 Using it explicitly like above doesn’t really help us much, although we have made sure to not introduce any errors related to the respective predictor being used as this is now fixed within the function calcRsq_pred1(). However, given that we now have a function which calculates the R-squared value between a fixed predictor and whatever response we give it, we can now use a functional such as apply() to calculate the relationship between the predictor and a bunch of responses. apply(resp, 2, calcRsq_pred1) ## resp1 resp2 resp3 resp4 ## 0.006369369 0.012977823 0.006369369 0.038727184 But why stop here? Taking advantage of sapply(), we can calculate every possible combination in one go. sapply(seq(ncol(pred)), function(i) { f &lt;- calcRsq(pred[, i]) apply(resp, 2, f) }) ## [,1] [,2] [,3] [,4] ## resp1 0.006369369 1.000000000 0.006326898 1.000000000 ## resp2 0.012977823 0.012414970 0.006192994 0.012414970 ## resp3 0.006369369 1.000000000 0.006326898 1.000000000 ## resp4 0.038727184 0.002312083 0.007719519 0.002312083 In words, we iterate over the columns of pred - seq(ncol(pred)), define a function f by setting the closure to use the column of the current iteration – f &lt;- calcRsq(pred[, i]), and apply this function f to all columns of resp - apply(resp, 2, f). The result is equivalent to what cor(resp, pred) produces. cor(resp, pred)^2 ## pred1 pred2 pred3 pred4 ## resp1 0.006369369 1.000000000 0.006326898 1.000000000 ## resp2 0.012977823 0.012414970 0.006192994 0.012414970 ## resp3 0.006369369 1.000000000 0.006326898 1.000000000 ## resp4 0.038727184 0.002312083 0.007719519 0.002312083 So you see that the combination of functionals and closures is a powerful, flexible and elegant way of generalizing computations. In fact, it is so flexible that we can now use the same functions for any data frames. df1 &lt;- data.frame(diamonds[, c(1, 5, 6)]) df2 &lt;- data.frame(diamonds[, 7:10]) sapply(seq(ncol(df1)), function(i) { f &lt;- calcRsq(df1[, i]) apply(df2, 2, f) }) ## [,1] [,2] [,3] ## price 0.8493305 0.0001133672 0.01616303 ## x 0.9508088 0.0006395460 0.03815939 ## y 0.9057751 0.0008608750 0.03376779 ## z 0.9089475 0.0090105434 0.02277947 This is especially valuable for large calculations that require iterating over a set of objects. A classic scenario for using closures in combination with functionals is to find a ‘best’ value, i.e. some parameter that optimizes a fit or something along those lines. A very detailed tutorial on how to use functional programming in R can be found in Hadley Wickham’s book Advanced R (Wickham 2014). This goes quite a bit deeper than what is outlined here and has a rather neat example of how to use functional programming to flexibly deal with encodings for missing data (e.g. -99, -999, -9999, etc.). References "],
["speeding-up-iteration-procedures.html", "4 Speeding up iteration procedures", " 4 Speeding up iteration procedures At this stage, we assume that you’ve grown familiar with R’s most relevant loop constructs, including for loops and the *apply family of functions. Yet, there’s two other packages we’d like to introduce within the scope of this short course due to their convenient performance when it comes to accelerating certain operations. But first things first… here’s the topics (and the related packages) that we’re gonna cover in the upcoming section on speeding up iteration procedures in R. Parallelization via doParallel (and foreach) Just a bit of theory ahead. “Speedup is a common measure of the performance gain from a parallel processor. It is defined as the ratio of the time required to complete the job with one processor to the time required to complete the job with N processors. […] In a machine where work can be dynamically assigned to available processors, it is attained as long as the number of pieces of work ready for processing is at least N.” (Denning and Tichy 1990) Or, to cut a long story short, if you’re performing one and the same iteration again and again, you may as well distribute an equal amount of sub-iterations to each processor available on your local machine. In theory, distributing an operation to N nodes would result in a n-fold speed gain. In this scope, we’ll have a brief look at R’s capabilities in terms of parallel processing using the doParallel package along with foreach (an other package to deal with loop constructs). Be aware, however, that there are plenty of opportunities to “go parallel” in R which you might want to have a look at. “Cross-lingual” programming via Rcpp The Rcpp package offers a seamless integration of C++ functionality in R. The underlying reason why such a thing exists in the first place is rather easy to explain: sometimes R code is just not fast enough. But don’t be afraid if you haven’t gotten in touch with C++ so far: we’re barely gonna scratch the surface of what is possibly when combining those two languages. In fact, our short excursion on the topic is merely meant to raise awareness of such things slumbering in the depth of the R universe. In case you’d like to learn more about the subject, we recommend to have a closer look at Hadley Wickham’s comprehensive introduction to High performance functions with Rcpp (Wickham 2014) or, if you’re a fan of hard copies, Dirk Eddelbuettel’s book about “Seamless R and C++ Integration with Rcpp” (Eddelbuettel 2013). References "],
["parallelization.html", "4.1 Parallelization", " 4.1 Parallelization As regards multi-core operations, there’s actually a whole bunch of packages that allow you to distribute particular processing steps to multiple cores on your local machine. Out of those, we decided to focus on doParallel (in conjunction with foreach) as it represents a straightforward solution that does not require the user to manually export objects from the global environment to the single nodes (e.g. required by par*apply from parallel). Getting started Let’s start straight away with detecting the number of cores available on your machine using parallel::detectCores(). In the end, there’s no use in trying to split a certain task into 5 parts when there are only 4 horses available, is it? 攼㹤愼㸰戼㹤攼㹤戼㸸㠼㸹 library(doParallel) ## detect number of cores nodes = detectCores() nodes ## [1] 4 In our case, there are 4 nodes available for parallelization among which iteration procedures may be divided. Note that this number typically varies between computers. Note also that library(doParallel) automatically attaches the foreach package which we will require later on, so there’s no need to manually load it. In fact, this would only be required if we decided to use foreach() on a single core, but for such operations, we highly recommend to use *apply instead. Next, let’s create a socket cluster for parallel processing via makeCluster(). Think of this as a set of workers among which a particular work step should be divided equally – just like in the image on the previous page. With registerDoParallel(), we tell R that it should use the cluster cl for any further multi-core operations performed with foreach(). ## create and register parallel backend cl &lt;- makeCluster(0.75 * nodes) registerDoParallel(cl) You will probably notice that we just told R to use 3 nodes to work with. ‘Why not use all 4 nodes?’, you might wonder. Well, parallel processing distributed among all nodes available tends to slow down your computer considerably. Since we’d possibly like to perform some other actions while R is occupied (e.g. browsing the internet, checking e-mails), it is wise to leave some remaining computational power for such operations as well. The foreach() syntax Now that we’re set up properly, it’s time to give our cluster something to work with. Remember the previous example on calculating the linear relationship between ‘carat’ and ‘price’ for each ‘cut’ quality iteratively? We’re gonna do the same thing again now except this time it’s foreach we’ll be working with. Similar to *apply, foreach() requires a set of input data and a function to perform on the very same. Have a look at the following example. ## calculate square root, return list foreach(i = 1:4) %do% sqrt(i) ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 1.414214 ## ## [[3]] ## [1] 1.732051 ## ## [[4]] ## [1] 2 Similar to lapply(), foreach() returns an object of class list by default. The function body, by contrast, is connected with the part on variable definition via the binary operator %do%. This might seem odd at first, but you will notice later on that this syntax comes in quite handy when forwarding foreach()-related operations to a parallel cluster. We may easily change the output type here by telling R how to .combine the results of the single iterations via ## calculate sqare root, return vector foreach(i = 1:4, .combine = &quot;c&quot;) %do% sqrt(i) ## [1] 1.000000 1.414214 1.732051 2.000000 Et voilà, we transformed the lapply-style list output of our loop into an sapply-style vector output. Task: apply lm() to selected columns using foreach() Let’s now come back to the diamonds dataset. Since you may just take the function body from the previous application with lapply(), it’s up to you to calculate the linear model between ‘carat’ and ‘price’ for each group of ‘cut’. Remember to split() the diamonds dataset first and pass the thus created list (with each list entry representing a group of uniform cuts) to foreach(). How not to use parallel features Let’s see how long this code chunk takes to perform. The microbenchmark package is just the right thing for such an operation. ## load &#39;microbenchmark&#39; package library(microbenchmark) ## load &#39;ggplot2&#39; and split &#39;diamonds&#39; dataset library(ggplot2) ls_diamonds &lt;- split(diamonds, f = diamonds$cut) ## speed test (this might take some time) microbenchmark({ foreach(i = ls_diamonds) %do% lm(carat ~ price, data = i) }, times = 20L) # number of times to evaluate the expression ## Unit: milliseconds ## expr ## { foreach(i = ls_diamonds) %do% lm(carat ~ price, data = i) } ## min lq mean median uq max neval ## 57.30358 58.29157 59.1779 58.94188 60.13144 62.36893 20 Hm, quite some time. Let’s see how long it takes on when using multiple cores. The only thing that’s required in order to let this operation run on multiple cores is to replace %do% with %dopar%. In doing so (and distributing the iterative lm() calculation to multiple cores), the operation should perform much faster, right? microbenchmark({ foreach(i = ls_diamonds) %dopar% lm(carat ~ price, data = i) }, times = 20L) ## Unit: milliseconds ## expr ## { foreach(i = ls_diamonds) %dopar% lm(carat ~ price, data = i) } ## min lq mean median uq max neval ## 95.02029 98.5116 252.5694 104.7193 144.6324 2749.455 20 Oops, what’s going on now? Obviously, this action doesn’t perform faster at all although we told R to run in parallel. In fact, this is a bad example for the parallelized use of foreach(). lm() is a highly optimized base-R function that performs quite fast without the need to go parallel. “With [such] small tasks, the overhead of scheduling the task and returning the result can be greater than the time to execute the task itself, resulting in poor performance.” (Weston and Calaway 2014) How to use parallel features Now that you’ve learned how not to use %dopar%, let’s see what a proper use would look like. ‘Proper’ in this prospect refers to a piece of code that is computationally expensive and needs to be repeated at least a couple of times. For example, let’s assume we wanted to separately predict ‘cut’, ‘color’ and ‘carat’ for each specimen from diamonds based on all the remaining variables. Using foreach() (or lapply()), the referring code would roughly look as follows. library(party) system.time({ ## conditional inference trees ls_ct &lt;- foreach(i = c(&quot;cut&quot;, &quot;color&quot;, &quot;carat&quot;)) %do% { # formula frml &lt;- as.formula(paste(i, &quot;.&quot;, sep = &quot; ~ &quot;)) # classification ctree(frml, data = diamonds, controls = ctree_control(testtype = &quot;MonteCarlo&quot;, nresample = 999, mincriterion = 0.999, maxdepth = 3)) } }) ## user system elapsed ## 34.79 0.00 34.81 ctree() from party performs rather slowly which is particularly owing to the nresample argument that tells the function to perform 1000 internal Monte-Carlo replications. Luckily, we can easily split this operation into 3 parts, i.e. one node takes ‘cut’ as response variable, another node ‘color’, and a third node ‘carat’ – at the same time! system.time({ ## conditional inference trees ls_ct &lt;- foreach(i = c(&quot;cut&quot;, &quot;color&quot;, &quot;carat&quot;), .packages = &quot;party&quot;) %dopar% { # formula frml &lt;- as.formula(paste(i, &quot;.&quot;, sep = &quot; ~ &quot;)) # classification ctree(frml, data = diamonds, controls = ctree_control(testtype = &quot;MonteCarlo&quot;, nresample = 999, mincriterion = 0.999, maxdepth = 3)) } }) ## user system elapsed ## 1.416 0.296 16.357 Of course, this is seconds we are talking about. Nonetheless, everyone of you will eventually end up with quite big datasets upon which computationally expensive operations need to be performed in an iterative manner. It might be hours or even days (trust me on this…) that one single operation takes to perform – and here, the %dopar% might come in quite handy. Closing a parallel backend One final remark on the proper use of parallel backends in R. When working on multiple cores, you can easily lose track of how many parallel backends, if any, you registered during your current session, especially when some error prevents your script from finishing. If you should ever find yourself in such a situation, do not hesitate to use showConnections() to print information on currently open connections to the R console. showConnections() ## description class mode text isopen ## 3 &quot;&lt;-DESKTOP-989003N:11679&quot; &quot;sockconn&quot; &quot;a+b&quot; &quot;binary&quot; &quot;opened&quot; ## 4 &quot;&lt;-DESKTOP-989003N:11679&quot; &quot;sockconn&quot; &quot;a+b&quot; &quot;binary&quot; &quot;opened&quot; ## 5 &quot;&lt;-DESKTOP-989003N:11679&quot; &quot;sockconn&quot; &quot;a+b&quot; &quot;binary&quot; &quot;opened&quot; ## 7 &quot;output&quot; &quot;textConnection&quot; &quot;wr&quot; &quot;text&quot; &quot;opened&quot; ## 8 &quot;&lt;-DESKTOP-989003N:11679&quot; &quot;sockconn&quot; &quot;a+b&quot; &quot;binary&quot; &quot;opened&quot; ## 9 &quot;&lt;-DESKTOP-989003N:11679&quot; &quot;sockconn&quot; &quot;a+b&quot; &quot;binary&quot; &quot;opened&quot; ## 10 &quot;&lt;-DESKTOP-989003N:11679&quot; &quot;sockconn&quot; &quot;a+b&quot; &quot;binary&quot; &quot;opened&quot; ## can read can write ## 3 &quot;yes&quot; &quot;yes&quot; ## 4 &quot;yes&quot; &quot;yes&quot; ## 5 &quot;yes&quot; &quot;yes&quot; ## 7 &quot;no&quot; &quot;yes&quot; ## 8 &quot;yes&quot; &quot;yes&quot; ## 9 &quot;yes&quot; &quot;yes&quot; ## 10 &quot;yes&quot; &quot;yes&quot; There’s 3 socket connections (i.e. cores) registered at the moment, just as we initially defined via registerDoParallel. In order to close these connections (which we recommend to explicitly do at the end of each parallelized R script), simply perform stopCluster(cl) which closes the implicitly created cluster (except for the ‘textConnection’ required to create this very bookdown document, of course 攼㹤愼㸰戼㹤攼㹤戼㸸㠼㸹). showConnections() ## description class mode text isopen ## 3 &quot;&lt;-DESKTOP-989003N:11679&quot; &quot;sockconn&quot; &quot;a+b&quot; &quot;binary&quot; &quot;opened&quot; ## 4 &quot;&lt;-DESKTOP-989003N:11679&quot; &quot;sockconn&quot; &quot;a+b&quot; &quot;binary&quot; &quot;opened&quot; ## 5 &quot;&lt;-DESKTOP-989003N:11679&quot; &quot;sockconn&quot; &quot;a+b&quot; &quot;binary&quot; &quot;opened&quot; ## 7 &quot;output&quot; &quot;textConnection&quot; &quot;wr&quot; &quot;text&quot; &quot;opened&quot; ## can read can write ## 3 &quot;yes&quot; &quot;yes&quot; ## 4 &quot;yes&quot; &quot;yes&quot; ## 5 &quot;yes&quot; &quot;yes&quot; ## 7 &quot;no&quot; &quot;yes&quot; References "],
["c-interconnectivity-via-rcpp.html", "4.2 C++ interconnectivity via Rcpp", " 4.2 C++ interconnectivity via Rcpp Prerequisites In order for Rcpp to work, we have to make sure your local system is capable of building packages. On Linux-based systems, this shouldn’t be much of a problem since things just generally tend to work whereas on Windows (or OS X), you will possible be required to install Rtools (or Xcode) to be able to compile C++ functions and make them available in R. Further information on package development prerequisites can be found here. You may easily check if everything works by running the following code chunk. ## load &#39;Rcpp&#39; package library(Rcpp) ## try to evaluate c++ expression evalCpp(&quot;1 + 1&quot;) ## [1] 2 If this expression does not evaluate to ‘2’, there’s something wrong with your local setup and you should possibly contact one of the lecturers for troubleshooting. How to make your C++ code available in R Rcpp offers two ways to import C++ functions into R, namely cppFunction() and sourceCpp(). While the former takes an entire C++ source code as input argument, the latter behaves very similar to base-R source() in the sense that it sources a code file (.cpp) and makes the functions included therein available in your global R environment. During this short overview, however, we’ll primarily focus on the first approach while the latter is introduced only briefly. No input, scalar output No matter which approach you will use in the end, Rcpp will take the C++ code, compile it and transform it into a proper R function. Imagine, for instance, the following code (which is heavily based on Hadley Wickham’s tutorial on High performance functions with Rcpp (Wickham 2014)). ## function that returns &#39;1&#39; cppFunction(&#39;int one() { return 1; }&#39;) one() ## [1] 1 Note that C++ requires you to specify the output type of one() which, in this particular case, is obviously an integer (int). Accordingly, R variables of type ‘numeric’, ‘character’ and ‘logical’ are referred to as double, String and bool in C++ language. Let’s move on to our next example. Scalar input, scalar output When working with C++ code, you are required to not only specify the output type of your function, but also the type(s) of the input argument(s). The next code chunk defines a function signC() that takes an integer input x and, depending on the arithmetic sign of x, returns one of ‘1’, ‘0’, and ‘-1’. ## function that returns 1 if &#39;x&#39; is positive, -1 if &#39;x&#39; is negative, 0 otherwise cppFunction(&#39;int signC(int x) { if (x &gt; 0) { return 1; } else if (x == 0) { return 0; } else { return -1; } }&#39;) signC(-10) ## [1] -1 Note also that if statements in C++ look very similar to their R equivalent. The only obvious differences are the need to explicitly include a return statement and the semicolons (;) terminating each line of code. Vector input, scalar output Although this seems hardly necessary, let’s assume for now that we wanted to rewrite the base-R sum function in C++. Rather than supplying a scalar input argument, we need the function to work with a vector of numbers for obvious reasons. Similar to the different scalar inputs depicted above, base-R ‘integer’, ‘numeric’, ‘character’ and ‘logical’ vectors are represented as IntegerVector, NumericVector, CharacterVector and LogicalVector in C++. This time, it also makes sense to define the input type as NumericVector and, accordingly, the output type as double since we’d possibly like to supply numbers with decimal places instead of raw integers. cppFunction(&#39;double sumC(NumericVector x) { int n = x.size(); double total = 0; for(int i = 0; i &lt; n; ++i) { total += x[i]; } return total; }&#39;) sumC(seq(0, 1, 0.1)) ## [1] 5.5 Matrix input, vector output Rcpp also comes with a number of so-called ‘sugars’ that help newcomers to find their way by providing C++-equivalents of a number of built-in R functions. A short overview of featured functions is e.g. given by Eddelbuettel and Francois (2011). Among others, these include math functions, e.g. abs, ceiling, floor, exp, log; scalar summaries, e.g. mean, min, max, sum, sd, `var; vector summaries, e.g. cumsum, diff; finding utilities, e.g. which_max, which_min, match; finding duplicates, e.g. duplicated, unique. In order to demonstrate the proper use of such ‘sugars’ and, at the same time, introduce NumericMatrix (NumericVector) as further input (output) variable types, let’s replicate the previous example on the use of apply to calculate mean values from each single variable column of the diamonds dataset using Rcpp functionality. For the sake of simplicity of this demonstration, let’s again focus on the numeric columns only (note also the use of sapply() to create an index vector of (non-)numeric columns). ## subset with numeric columns only num_cols &lt;- sapply(1:ncol(diamonds), function(i) { is.numeric(data.frame(diamonds)[, i]) }) diamonds_sub &lt;- as.matrix(diamonds[, num_cols]) ## c++-version of &#39;colMeans&#39; cppFunction(&quot;NumericVector colMeansC(NumericMatrix x) { // number of rows and columns int nCol = x.ncol(); int nRow = x.nrow(); // temporary variable of size nrow(x) to store column values in NumericVector nVal(nRow); // initialize output vector NumericVector out(nCol); // loop over each column for (int i = 0; i &lt; nCol; i++) { // values in current column nVal = x(_, i); // store mean of current &#39;nVal&#39; in &#39;out[i]&#39; out[i] = mean(nVal); } return out; }&quot;) means &lt;- colMeansC(diamonds_sub) names(means) &lt;- colnames(diamonds_sub) means ## carat depth table price x ## 0.7979397 61.7494049 57.4571839 3932.7997219 5.7311572 ## y z ## 5.7345260 3.5387338 ## speed check microbenchmark( val_apply &lt;- apply(diamonds_sub, 2, mean), val_cpp &lt;- colMeansC(diamonds_sub) , times = 20L) ## Unit: milliseconds ## expr min lq mean ## val_apply &lt;- apply(diamonds_sub, 2, mean) 5.142534 5.187900 5.385949 ## val_cpp &lt;- colMeansC(diamonds_sub) 1.158980 1.170475 1.242834 ## median uq max neval cld ## 5.339393 5.476106 6.158233 20 b ## 1.248274 1.308215 1.320326 20 a ## similarity check identical(val_apply, means) ## [1] TRUE It’s milliseconds we are talking about here, but still - colMeansC() runs more than 5 times faster as compared to the apply() approach! What’s the point of that? You might guess that we did not decide to include this chapter on C++ interconnectivity just for fun. The actual reason is that C++ code performs much faster as compared to R when it comes to for() loops. Without going too much into detail, one of the underlying reason is the very efficient memory management of the C++ language as compared to the massive overhead that R produces during each intermediary step. But find out for yourselves… Task: sumR() vs. sumC() Write a function sumR() (do not use the built-in sum function) as an equivalent to the above sumC() function and have a look at the time it takes to run sumR(1:1e4) using system.time() (or microbenchmark()). ## speed check microbenchmark( sum(1:1e4), # built-in `sum` function sumR(1:1e4), # base-R version sumC(1:1e4) # rcpp version , times = 100L) ## Unit: microseconds ## expr min lq mean median uq max neval ## sum(1:10000) 13.138 14.3700 15.60138 14.781 15.602 30.792 100 ## sumR(1:10000) 557.936 560.4000 675.62845 562.247 580.106 6391.835 100 ## sumC(1:10000) 29.560 32.8445 34.59753 34.076 35.308 51.319 100 ## cld ## a ## b ## a As you can see, sumC() runs more than 40 times faster than sumR() and, at the same time, takes only slightly longer as compared to the highly optimized (because vectorized) built-in sum() function. You cannot imagine what’s possible with Rcpp when it comes to more complex operations! A short note on the use of sourceCpp For such short operations, the use of cppFunction() seems reasonable. However, we recommend to use sourceCpp() when it comes to more complex C++ functions. Take, for example, the following peace of C++ code that reproduces the built-in cor() function. cppFunction(&#39;double corC(NumericVector x, NumericVector y) { int nx = x.size(), ny = y.size(); if (nx != ny) stop(&quot;Input vectors must have equal length!&quot;); double sum_x = sum(x), sum_y = sum(y); NumericVector xy = x * y; NumericVector x_squ = x * x, y_squ = y * y; double sum_xy = sum(xy); double sum_x_squ = sum(x_squ), sum_y_squ = sum(y_squ); double out = ((nx * sum_xy) - (sum_x * sum_y)) / sqrt((nx * sum_x_squ - pow(sum_x, 2.0)) * (nx * sum_y_squ - pow(sum_y, 2.0))); return out; }&#39;) Quite confusing, isn’t it? Not the least because the inline C++ is not formatted properly. Luckily, RStudio comes with a C++ editor that allows you to write stand-alone .cpp functions – including code formatting! For that purpose, select ‘C++ file’ from the top-left drop-down menu and paste the code that we initially passed as character input to cppFunction(). In order to ensure compatibility with Rcpp and make the C++ function available in R, we need to add a header to our .cpp file (see below). In the end, this should look as follows. #include &lt;Rcpp.h&gt; using namespace Rcpp; // [[Rcpp::export]] double corC(NumericVector x, NumericVector y) { int nx = x.size(), ny = y.size(); if (nx != ny) stop(&quot;Input vectors must have equal length!&quot;); double sum_x = sum(x), sum_y = sum(y); NumericVector xy = x * y; NumericVector x_squ = x * x, y_squ = y * y; double sum_xy = sum(xy); double sum_x_squ = sum(x_squ), sum_y_squ = sum(y_squ); double out = ((nx * sum_xy) - (sum_x * sum_y)) / sqrt((nx * sum_x_squ - pow(sum_x, 2.0)) * (nx * sum_y_squ - pow(sum_y, 2.0))); return out; } Save the file in src/corC.cpp, for example, and go back to R. Then run ## source &#39;corC&#39; function (remember to adjust the path) sourceCpp(&quot;src/corC.cpp&quot;) ## correlation of &#39;carat&#39; and &#39;price&#39; microbenchmark( cor(diamonds$carat, diamonds$price), corC(diamonds$carat, diamonds$price) , times = 20L) ## Unit: microseconds ## expr min lq mean median ## cor(diamonds$carat, diamonds$price) 776.349 795.2335 889.1878 852.5055 ## corC(diamonds$carat, diamonds$price) 720.924 787.0230 837.4791 809.8080 ## uq max neval cld ## 918.8085 1271.881 20 a ## 851.8895 1225.078 20 a Wow, corC() performs even faster than the built-in and highly optimized cor() function – at least on my machine. Just imagine the speed gain as compared to a self-written corR() function! References "],
["final-remarks.html", "5 Final remarks", " 5 Final remarks I sincerely hope that this tutorial provides some useful parts to enhance your data analysis workflow. For a more general approach to software design (every function that you write is a software!), have a look at Wilson (2014). The sources of this tutorial are hosted at GitHub. Bug reports or feature requests are always welcome and should be filed here. In case you want to provide feedback, don’t hesitate to contact me at florian.detsch{at}staff.uni-marburg.de. References "],
["references.html", "References", " References "]
]
